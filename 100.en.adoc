= THE_TOOL – A GBIF DNA metabarcoding data conversion tool
:toc: left
:toclevels: 4

== Introduction

=== Overview

GBIF.org offers access to extensive biodiversity data, mainly focusing on where and when species have been observed. These records, sourced from collections, surveys, and citizen science, include species names, dates, locations, and occasionally additional details like habitat and images. For several years, it has been possible to publish DNA-associated biodiversity data to GBIF, including data from environmental DNA (eDNA) surveys and other molecular data sources for several year, and in 2021, the first version of a practical dna-publishing guide was published. During consultations with the GBIF network and the research community in 2022 and 2023, it became clear that tools for easier data formatting and publishing were needed for DNA metabarcoding data.

the_tool intends to offer a tool that allows users with minimal prior knowledge of data standards, GBIF and DNA metabarcoding data to reshape traditional data formats – OTU table type datasets – from the DNA metabarcoding community into standardised datasets possible to publish to GBIF. 


=== Target audiences

This guide is designed for anyone interested in publishing DNA metabarcoding data on GBIF or similar biodiversity platforms, catering to both seasoned GBIF network members and researchers new to GBIF and the data publishing process.

All new users are encouraged to review and go through both quick start guides to familiarize themselves with THE_TOOL and the processing concepts, and then consult other relevant sections of the guide and ressources as needed. 

=== Essential terms

There is a full glossary in the end of the document, but here er list a few essential terms for quick reference to new users. `decimalLatitude`. 

* https://dwc.tdwg.org/[Darwin Core^] (Abbr: DwC): The name of the data standard used by GBIF.
* Darwin Core term: a standardised field name (e.g. https://dwc.tdwg.org/list/#dwc_decimalLatitude[`decimalLatitude`^] is the official DwC term for latitude).
* Occurrence: the GBIF and DwC word for a registration/detection of a species in space and time.
* Occurrence core: This is the part of DwC that includes all the central information (fields) on biological occurrences in GBIF (e.g. spatiotemporal data, taxonomy, etc), also for eDNA data.
* dna-derived data: An extension to Occurrence core to capture information relating to DNA (e.g. primers, the sequence, sequencing platform, etc.). This extension is based on the MIxS standard used by the "GenBanks". 
* OTU (Operational Taxonomic Unit): In this context it referes to to molecular units / molecular species detected in an eDNA metabarcoding study. OTUs are often a cluster of higly similar species represented by one selected sequence type.
* ASV: Amplicon sequence variant. A variant of OTUs where where only denoising of the sequence data, but no clustering, has been performed. For simplicity we refer to OTUs in general to cover also ASVs.
* OTU table: spreadsheet that holds the number of sequencing reads detected of each OTU/sequence in each sample. Here used broadly to also cover ASVs.
* Darwin Core archive (DwC-A): A standardized format for sharing biodiversity data – a ZIP file that contains one or more data files along with a descriptor file that explains how the data files are organized.
* Endpoint: In the context of GBIF, an "endpoint" refers to a URL or web address where a DwC-A can be accessed through the internet, and indexed by GBIF.

== Quick Start Guides

The two quick start guides should ideally introduce most users to all features necessary to know for being able to adapt own or existing datasets to process in the tool. Start with the simple and then move to the advanced.

=== Simple Quick Start Guide

This [.underline]#simple# quick start guide handles the processing of a minimal example dataset  with only essential explanation. It requires little or no prior knowledge about eDNA data, GBIF and DarwinCore. The intention is to familiarize the user with THE_TOOL to get basic ideas of how to adapt real datasets for processing in the tool. We recommend to follow the advanced quick start guide as the next step.

==== Simple example dataset [[q1_test_data]]

This quick start uses a very simple and minimalistic example datset with only 5 samples and 4 OTUs.

. Go to https://xxxx[this example dataset^] and download it (File > Download > Microsoft Excel).
. Save the file to your computer.
. (Optional) Briefly explore the structure of the example data in Excel or another app (or see detailed explanations in the section XXX ).
** The *OTU_table* sheet in the template is the OTU table. Column headers are the IDs of the 5 samples in the dataset. Row names are IDs of the 4 OTUs. Cells contain sequence read counts.
** The *Taxonomy* sheet contains the IDs of the OTUs referring to the rownames in the *OTU_table* sheet, and OTU data: The sequence, and taxonomic information derived from some reference database.
** The *Samples* sheet contains tha ID of the samples referring to the column names in the *OTU_table* sheet, and sample metadata: sample ID, latitude, longitude and date.
** The *Study* sheet contains values that are the same for the whole dataset ("global values"), in this case: the barcoding regions used, primer sequences, and primer names.
+
NOTE: all the fields (columns) of this example data are already using the 

==== Log into THE_TOOL

. Go to the website of https://edna-tool.gbif-uat.org/[THE_TOOL^].
. Log in
+
NOTE: If you do not have an account, the login prompt will link you to the sign-up form.

. Press *New Dataset* in the upper part of the page.
+
This opens <<q1_upload>> – the first step of the data processing.


==== Upload data (step 1) [[q1_upload]]

image::process_step_1.png[]

. Drag the <<q1_test_data,test data>> to the upload area, or click and select the file.
. Give the dataset a nickname (e.g. "my_first_test")
. Press *Start Upload*.
+
A green icon will indicate that the data looks OK according to some very basic data checks.
. Press *Proceed* to go to <<q1_mapping>>.

==== Map terms (step 2) [[q1_mapping]]

On this page you tell the tool what the fields in the data mean. As this test dataset already uses Darwin Core field names, no manual mapping is needed.


TIP: Press *how to use this form* to get a guided tour of this page.

* The upper section maps our sample data fields to Darwin Core terms (first column), automatically identifying and mapping four fields from the *Samples* sheet (second column) and five fields from the *Study* sheet with global values (third column) to their identically named Darwin Core counterparts. (e.g. the field containing sampling dates was called _eventDate_ in the uploaded data corresponding exactly to the Darwin Core term `eventDate`, and the field _pcr_primer_forward_ corresponding to `pcr_primer_forward`).

* The second section does the same for Taxonomy and sequence related information, auto-mapping four fields from the *Taxonomy* sheet to the identically named Darwin Core terms.

. Press *Proceed* to save the mapping and go to <<q1_process>>

==== Process data (step 3) [[q1_process]]

. Press *Process data*.
+
This produces standardized intermediate files in the BIOM format
+
NOTE: The option *assign taxonomy* uses the https://www.gbif.org/tools/sequence-id[GBIF Sequence ID tool^] to assign taxonomy to the sequences. This overwrites any taxonomy provided in the data.

. (Optional) Briefly check that number of samples and taxa are as expected (here: 5 samples and 4 taxa).
. Press *Proceed* to go to <<q1_review>>

==== Review (step 4) [[q1_review]]

Here the data can be explored to check that everything is OK. This step is mainly intended as a sanity check of the data to ensure that control samples have been removed, and that the mapping is as expected.


. (*Optional*) Check the data.
.. Check the map and verify that the samples are placed geographically where expected (Northern part on Denmark). 
.. Check the taxonomic barchart to ensure that taxonomic composition is as expected.
.. Check ordination plots (PCoA/MDS) for outliers (any control samples that should have been excluded?).
.. Select single samples from the map or chart and explore their metadata and taxonomy in the panel to the right.
. Press *Proceed* to go to <<q1_metadata>>


==== Add metadata (step 5) [[q1_metadata]]

On this page dataset metadata is added in a minimalistic form.


. (Mandatory) Add a meaningful title (e.g. “my first test dataset”).
. (Mandatory) Select a licence.
. (Mandatory) Add contact information - minimum is email and orcid
+
NOTE: use e.g. 1111-2222-3333-123X as dummy orcid if you wish.
. Leave the other fields empty.
. Press *Proceed* to continue to <<q1_publish>>


==== Publish (step 6) [[q1_publish]]

This last page of the process produces a so-called Darwin Core archive (a zip file) that can be published directly to the GBIF test environment (UAT) from THE_TOOL. This archive can also be published properly to GBIF.org.


. Press *Create DWC archive*.
+
This creates the Darwin Core Archive from the data.
. Press *Publish to GBIF test environment (UAT)*.


A prompt will inform that it takes some minutes before the data is fully ingested and will show up with all samples in the GBIF test environment. A link to the dataset in the test environment will appear next to the *Publish* button.

[start=3]
. Click on your username in the top right. Here you can:
.. see your datasets,
.. access them on the test environment (UAT), and
.. modify and publish updated/new versions.

You should now have a first basic ideas of how the tool works and how you may adapt your own datasets to the template and use THE_TOOL. It is highly recommended to now go through the advanced quick start.


If you end up with a dataset suitable for publication to GBIF.org, go to <<publishing_to_gbif>>.


=== Advanced Quick Start Guide

This [.underline]#advanced# quick start guide handles the processing of a realistic example dataset with more detailed explanations of the steps and options. You should be able to follow this guide if you already used the simple quick start, but a deeper understanding of all aspects may require further reading about eDNA metabarcoding, dna-derived data, GBIF and Darwin Core data standards. The intention is to familiarize the user with the wider possibilities of THE_TOOL to be able to adapt real datasets for processing in the tool.

NOTE: Some comments and notes are not repeated from the simple quick start.


==== Download example dataset

This quick start quide uses a slightly modified version of a real dataset. It is from a eDNA metabarcoding study where DNA was extracted from sea water samples and amplified and sequenced for the 12S gene region. This version has been modified slightly from the original [xxx] to be able to illustrate some features in the tool and workflow, by adding fictional data and some errors.


. Go to https://xxxx[this example dataset^] and download it (File > Download > Microsoft Excel).
. Save the file to your computer.
. Explore the structure of the template and example data in Excel or another app.
+
NOTE: there is a more detailed description of this example dataset here [XXX].

** The *OTU_table* sheet in the template is the OTU table. Column headers _(BAR.1, BAR.2, BAR.3, ..._) are the IDs of the 69(70) samples in the dataset. Row names (_ASV_1, ASV_2, ASV_3, ..._) are IDs of the 563 OTUs (here ASVs). Cells contain sequence read counts.
** The *Taxonomy* sheet contains the IDs of the 563 OTUs referring to the rownames in the *OTU_table* sheet, and OTU data: The sequence, and taxonomic information derived from comparing the sequences against NCBI GenBank. Some fields are using DwC terms others are not yet standardised.
** The *Samples* sheet contains the IDs of the 69 samples referring to the column names in the *OTU_table* sheet, and some sample metadata: spatiotemporal data, date, etc. Some fields are using DwC terms others are not yet standardised.
** The *Study* sheet contain "sample information" that are the same for the whole dataset - e.g.: primer information, sequencing platform.


==== Log into THE_TOOL

. Go to the website of https://edna-tool.gbif-uat.org/[THE_TOOL^].
. Log in
. Press *New Dataset* in the upper part of the page.


==== Upload data (step 1)

image::img/process_step_1.png[]

. Drag the <<q1_test_data,test data>> to the upload area, or click and select the file.
. Give the dataset a nickname (e.g. "my_advanced_test")
. Press *Start Upload*.
+
A green icon will indicate that the XLSX format is detected and OK according to some very basic data checks.

You will get a warning that one of the columns (samples) in the OTU table does not have a corresponding row in the sample sheet.

image::img/advanced_example_upload_warning.png[]

. Open the data viewer by clicking on the eye icon next to the uploaded dataset
+ 
Here you can see and verify the structure and content of the four sheets from the uploaded excel file.
. Scroll all the way to the right in the OTU table, and notice that there the last sample called _NEG_, probably something we do not want.
. Click on the "Samples" tab, and scroll down and notice that this _NEG_ sample is missing (has been removed on purpose) from the sample sheet. We will leave is like that.
+
NOTE: The tool will only include samples that are present in both *Samples* and *OTU_table*, so a trick to ignore control samples is to remove them just in one place. 
. Close the viewer by pressing *Back*.
. Press *Proceed*


==== Map terms (step 2)

On this page you tell the tool which DwC terms correspond to which fields in the uploaded data.


TIP: Press *how to use this form* to get a guided tour of this page.

TIP: Press *Save mapping* once in a while to make sure that you do not get logged out and lose your work.

*First inspection*

. Inspect the overall structure and information on the page.
.. The upper section named *Sample* maps our sample data fields to Darwin Core terms (first column), automatically identifying and mapping four fields from the *Samples* sheet (second column) and five global fields from the *Study* sheet (third column) with their identically named Darwin Core counterparts.
.. The second section named *Taxon* does the same for taxonomic and sequence related information, auto-mapping four fields from the Taxon sheet to identically named Darwin Core fields.
.. The last section *Unmapped fields* lists all the fields in the uploaded data, that has names the tool do not easily recognize. Below there is an option to put unmapped fields into so-called *Extended Measurement Or Facts*.
. Press "Save Mapping" and see how you get a warning about how some essential fields have not ben mapped.

*Completing the mapping*

Starting from the top with *Sample* information, we see:

* `id` was correctly identified and mapped.
* `eventDate` was not found in the uploaded data, but the tool suggest to use _date_.
** click on _date_ to make this mapping.
* `decimalLatitude` was correctly mapped.
* `decimalLatitude` was not found in the uploaded data, and there is no suggestion.
** click on the empty field next to `decimalLatitude`, and inspect the field names from the uploaded data and notice how the latitude field was simply misspelled _ecimalLatitude_. Select it to make the mapping.
* _target_gene_ was correctly set to "12S" which was picked from the *Study* sheet containing terms with global values.
* `otu_db` also got a value "NCBI nt" from the *Study* sheet.
* `island` was mapped to _island_.
* `env_medium` and the last terms of the sample section were also automatically were mapped to fields in the *study* sheet.

Now, going down to the *Taxon*, we see:

* `id` and all the taxonomic levels were mapped automatically.
* `DNA_sequence` was not mapped automatically, but suggests _sequence_.
** click on "sequence" to map that.

Now, going down to *Unmapped fields*, we see a series of fields (_SiteType, Depth, Location, run_accession, sample_accession, salinity_) in the uploaded data, that were not automatically identified and mapped to any Darwin Core terms.

NOTE: A detailed description the fields of the example dataset is in the section XXX.

We expect (or know) that Darwin Core can accomodate several of these un-mapped fields, and we also want to supply some global information (e.g. country), which was not included in the uploaded data.

. Go to the last part of the *sample* section.
. Click on *Add mapping for another sample field* and look at the list of available terms.
. We wish to find some standard field to map to our field _Location_.
.. start typing "Loca" and select `verbatimLocality`.
.. click *Add field*, and see how the field is added to the list of terms.
.. Now, select our field _Location_ to map it.
. Now, we wish to map the fields with information on the corresponding sequencing files in INSDC (ENA/SRA), and follow the GBIF recommendations for which fields to use for this purpose:
.. Find and add the term `associatedSequences` and map it to our field _run_accession_.
.. Find and add the term `materialSampleID` and map it to our field _sample_accession_.
. To make the dataset more well documented, we will include some information that we have, but was not included in the upload.
.. All sample were from Ecuador. Add the term `Country` and type "Ecuador" in the *Add default value*.
.. We also know that all samples were from the upper layers of oceanic water.
... Add the term `env_broad_scale`.
... see how it is now possible to Browse the ENVO Ontology. Click and search for "epipelagic" and select the "oceanic epipelagic zone biome" with OBO ID "ENVO:01000035".
+
NOTE: this is also how the term `env_medium` (above) is filled out from the same ontology, but this was supplied in the uploaded data.

Now, going down to *Unmapped fields*, we see that only a few fields remain unmapped. We really want to map _salinity_, but there is no standard field for that. So we will put that into *Extended Measurement Or Facts*.

. click on *salinity* from the row of unmapped fields and see how it is transferred to the the section below as a new entry.
. We know that the measurement unit is "PSU", so we add that.

Now, the mapping is complete.

NOTE: All available standard fields (from Occurrence Core, and the dna-derived extension) can be included in the upload files, and if spelled correctly no manual mapping is needed.

. Press *Proceed*.

==== Process data (step 3) [[q2_process]]

. Press *Process data*.
+
The tool goes through a series of steps which will be indicated as succesful with a green tick-mark, and finally produces standardized BIOM files, which the tool uses as an intermediate file format.
+
NOTE: You will get a warning that "NEG in the OTUTABLE are not present in the SAMPLETABLE". We knew that and kept it like that to exclude this Negative control from the final data.
+
NOTE: The option *assign taxonomy* uses the https://www.gbif.org/tools/sequence-id[GBIF Sequence ID tool^] to assign taxonomy to the OTUs by comparing the sequences with a reference database. This overwrites any taxonomy provided in the data. If you wish to try it here, you will see that the current 12S reference database cannot assign taxonomy to a number of the sequences in this dataset project (all the non-fish). This guide assumes that you used the taxonomy in the uploaded data.

. Check that number of samples and taxa are as expected (here: 69 samples and 563 taxa).
. Press *Proceed*

==== Review (step 4)

Here the data can be explored to check that everything is OK. The options in this step are intended as sanity checks of the data to ensure that e.g. negative control samples have been removed, and that the mapping is as expected.

. Check the data.
.. Check the map and verify that the samples are placed geographically where expected (Around Galapagos Islands).
.. Check the taxonomic barchart to ensure that taxonomic composition is as expected.
... try some of the other options (e.g. Absolute read abundance).
.. Check ordination plots (PCoA/MDS) for outliers (any control samples that should have been excluded?).
.. Select single samples from the map or from charts and explore their metadata and taxonomy in the panel to the right.
. Press *Proceed*


==== Add metadata (step 5) [[q1_metadata]]

On this page, dataset metadata (dataset description, persons and affiliations, etc.) is added in a minimalistic form.

TIP: toggle the "Show help" to get guidance text for the fields.

. Add a meaningful title (e.g. “Fish and other vertebrates detected in sea water from the sea around Galapagos Islands, Ecuador; inferred from 12S DNA metabarcoding data with primers targeting elasmobranch.”).
. Select a licence.
. Give as rich a dataset description as you can.
. Add contact information - minimum is email and orcid.
+
NOTE: use e.g. 1111-2222-3333-123X as dummy orcid if you wish.
. Fill out the other fields as good as possible.
. See below XXX for best practice on filling out this form.
. Press *Proceed*.


==== Export (step 6) [[q1_publish]]

This last page of the process produces a Darwin Core Archive that can be published directly to the GBIF test environment (UAT) from THE_TOOL. This archive can also be published properly to GBIF.org eventually.

. Press *Create DWC archive*.
+
This creates the Darwin Core Archive from the data, going through a series of steps, that will be indicated as succesful with a green tick-mark.
. Press *Publish to GBIF test environment (UAT)*.

A prompt will inform that it takes some minutes before the data is fully ingested and will show up with all samples in the GBIF test environment (and the map will only appear the next day). A link to the dataset in the test environment will appear next to the *Publish* button.

. Explore the dataset in the test environment
. Ensure that all information and data is processed and displayed appropriately.

You should now have an good idea of how the tool works and how you may adapt your own datasets to the template and use THE_TOOL.

Be sure to check the best practices from the Detailed user guide below.


If you end up with a dataset suitable for publication to GBIF.org, go to <<publishing_to_gbif>>.


== Detailed User Guide

=== User interface overview

=== Data Preparation

=== Dataset Templates

=== Example Datasets

=== Upload

=== Mapping

=== Processing

=== Reviewing

=== Adding Metadata

=== Publish

=== 

== The OTU table as a publishing model [[otu_table]]



== Publishing datasets from THE_TOOL to GBIF.org [[publishing_to_gbif]]

== FAQ

*Q. What is the state of this tool?*  

*A.* This is a prototype, and is beeing continuously developed. This means, that you may encounter bugs and problems that we have not yet addressed. You will be able to make a Darwin Core archive and download it, but you will not be able to publish it directly to GBIF.org through THE_TOOL. If you encounter bugs, inconveniences, have concrete input or want to request a feature, please make a github issue using the links on website of THE_TOOL.

*Q. What does THE_TOOL do?*

*A.* It helps format a DNA metabarcoding dataset (OTU table style) to be published on GBIF.org without the user having to learn Darwin Core terms and know a lot about data standardisation and reformating. It performs a transformation of the familar OTU table (with associated sample info & taxonomic/sequence informantion) into a tall table, where each row reflects one occurrence – a taxon (sequence/OTU/ASV) in time and space – and facilitates the mapping/renaming of user-named field names to the biodiversity standard, DarwinCore. These are all steps that can be done manually following the DNA publishing guide [LINK], but THE_TOOL makes it easier.

*Q. Who can use the tool?*

*A.* Anybody. 

*Q. Are there templates?*

*A.* Yes, there are a few templates and also example datasets. See above (XXX).

*Q. What kind of data can be published/submitted using this tool?*

*A.* This tool processes an OTU table so the data can be published to GBIF.org. With an OTU table we think of a table containing some amplified marker gene sequences (ASVs/OTUs) and their sequence abundance in a set of samples. Each sample corresponds to an environmental sample or bulk sample (air, soil, water, faeces, insect trap homogenate, gut contents, ...), from which DNA has been extracted. A selected genetic region (barcode region) has been amplified with selected primers and sequenced on a high throughput seqeuncing platform like Illumina MiSeq.

*Q. Can the tool be used metagenomic datasets?*

*A.* No. However note, that there is a confusion about the terms "metagenomic" and "metabarcoding". Metagenomic data sequences and captures all genetic material from an environmental sample, often with so-called shotgun sequencing. Metabarcoding data sequences specific selected DNA regions often called barcoding regions (e.g. CO1, ITS, 18S, 16S) to identify species in a sample, focusing on community composition. So, although the microbial research community often labels 16S amplicon sequencing (16S metabarcoding) as "metagenomic", that type of data would be suitable for processing in this tool, as it is associated with 16S sequences only. 

*Q. What kind of DNA metabarcoding samples are acceptable to publish on GBIF.org?*

*A.* eDNA metabarcoding based data from all environmental samples (soil, air, water, dust, etc) as well as bulk samples of small organisms (e.g. from malaise trap) are acceptable. Heavily manipulated/treated environmental samples may not reflect real biodiversity and deemed as irrelevant from a biodiversity perspective. Use your judgement.

*Q. Which markers/barcodes (COI, ITS, 16S,..) does GBIF and the tool support?*

*A.* It is possible to publish data based on amplification and sequencing of any amplified barcoding region. 

*Q. Should sequences be trimmed?*

*A.* Primers, adapters and tags, etc should always be removed from sequences. If you have trimmed your sequences further (e.g. trimming away the end of 5.8S and start of 28S from ITS2 data), then that is also acceptable, but not a requirement.

*Q. Should sequences be clustered into OTUs?*

*A.* 100% identical sequences should always be collapsed (dereplicated), and futher clustering, denoising and compression may be relevant depending of sequencing platform and bioinformatic tools used. If using e.g. the Illumina MiSeq platform, we recommend sharing unclustered (but denoised) amplicon sequence variants (ASVs). This approach keeps the data maximally interoperable with data from other studies, compared to clusting into broader (e.g. 97% culstering) OTUs, where centroids (the variant picked to represent an OTU) of almost similar OTUs may have been picked differently between datasets and algorithms.


*Q. Should sequence read abundance be converted to relative abundance?*

*A.* No. GBIF recommends to share detected absolute sequence read abundance (detected number of reads of each ASV/OTU in each sample). The tool will automatically calculate the total number of reads per sample and relative abundance, so that future users will have the option to filter on both absolute and relative abundance.

*Q. Should samples be resampled/rarefied to even sequencing depth?*

*A.* No. When doing metabarcoding, researchers are often resampling the OTU tables to achieve even sequencing depth (same total number of reads per sample) to standardise sampling effort across samples. GBIF recommends to share detected absolute abundances (number of reads per ASV/OTU in each sample). The tool will automatically calculate total number of reads per sample and relative abundances, so that future users have the option to filter on both absolute and relative abundances. Users downloading whole datasets will be able to do this resampling themselves if they wish.

*Q. Should negative controls, positive controls, blanks and failed samples be removed from the dataset?*

*A.* Yes. Only share data from real environmental samples producing data that seems trustworthy should be shared. NB: The tool only includes samples that are present in both the sample data AND the OTU table - i.e. it automatically discards samples that are absent from either table. So, removing controls from the sample-list is an easy way to do that.


*Q. Should I remove singletons, infrequent or low abundant sequences?*

*A.* No. There may be a good reason to remove low abundant sequences, singletons, infrequent sequences in some studies. But GBIF does not recommend any default removal of singletons, infrequent og low abundant sequences.

*Q. Should data from replicates be merged?*

*A.* Maybe. Do what makes the data most suitable for reuse in biodiversity studies. If replication (multiple samples, DNA extractions, PCRs) was used to reduce stochasticity, then (bioinformatic) merging of replicates may be a good choice.

*Q. What if there are several versions of an OTU table?*

*A.* Only one verison of the OTU table should be shared. Sometimes several version of an OTU table exist - e.g. clustered at different thresholds, removed non-target species and suspected contaminants - or split it into several tables with different taxonomic scopes. GBIF recommends to share the most inclusive version, including everything detected.

*Q. Should data from suspected contaminants be removed?*

*A.* Yes. Some sequences/OTUs may be suspected contamination (e.g. DNA from human and classical food items like tomato, potato, chicken, etc.). We recommend to remove these if they can be identified. Only taxa/OTUs that are present in both the taxon table AND the OTU table will be processed. So, removing suspected contaminats from the taxon information is an easy way to do that.

*Q. Should non-target sequences be removed?*

*A.* Not necessarily. Some sequences/OTUs are perceived as non-target sequences - e.g. if mammals are detected in a study using fish-specific primers. However, most of those non-target sequences may still be biodiversity relevant data seen in a larger perspective. Also, such custom filterings of data may actually make the data less compatible with similar datasets produced with the same primers, and it makes the calculation of relative read abundances flawed. So, GBIF generally encourages not to remove non-target sequences, unless they are obviously contaminations or otherwise untrustworthy.

*Q. Should taxonomy be assigned to sequences?*

*A.* Not necessarily. Currently GBIF identifies/indexes data based on the taxonomy you provide. If only the sequence is provided, the inferred occurrences will be stored under the label "incertae sedis" for now. However, the presence of the sequence will make it possible to assign taxonomy at a later stage. GBIF aims to provide the possibility of automatic updating of sequence based identification (see above). The tool currently also allows assigning of taxonomy for a few genetic markers and organism groups.

*Q. How should taxonomy be assigned to sequences?*

*A.* There are many reference databases and tools for assigning taxonomy to sequences, and reference databases are continuously being improved and changed. GBIF does not recommend any particular tool or pipeline. Use what is appropriate for the data. GBIF provides a sequence annotation tool for some markers. You can use that if you wish. The sequence ID tool is also built into this eDNA data converter tool as an option during the processing step, but as this step takes time you may want to use the sequence ID tool alone before using using this conveter. [NB: In the long term GBIF hopes to be able to continuously reannotate sequence based data to ensure consistency across datasets and time. GBIF will however keep original taxonomic identifications provided by the user to ensure traceability.]

*Q. How should I provide the taxonomic information when I submit my OTU data to GBIF?*

*A.* Take a look at the template (link above XXXX).

*Q. Should I share sequences that cannot be taxonomically identified?*

*A.* Yes. By default all OTUs/ASVs should be shared. Sequences that cannot be reliably identified to species level (or to genus, or any taxonomic level at all) generally reflect the fact that reference databases are incomplete and/or not 100% curated. However, as reference databases are continuously improved, many sequences will be possible to receive improved taxonomic affiliation. So please provide all sequences.

*Q. Will GBIF make sure that the taxonomy is updated?*

*A.* Hopefully yes. For many barcoding regions and taxonomic groups, reference databases are incomplete and partially incorrect, but continuously improved. Thus, taxonomic identifications based on comparison with reference databases often reflect the current state of the database used. In the long term GBIF aims to continuously reannotate sequence based data to ensure consistency across datasets and time. GBIF will keep original taxonomic identifications provided by the user to ensure traceability.

*Q. How does GBIF ensure fitness for reuse and interoperability of data?*

*A.* In the long term GBIF aims to continuously re-annotate sequence based data to ensure consistency across datasets and time. GBIF will however keep original taxonomic identifications provided by the user to ensure traceability. GBIF is also working on better tools for searching for and filtering of sequence based data.

*Q. Can the tool be used to just to make a Darwin Core archive?*

*A.* Yes. The tool can be used to produce a Darwin Core archive. This darwin core archive can then be published to GBIF, OBIS or another research infrastructure through another publishing process.

*Q. Can the tool be used to just to make a BIOM file?*

*A.* Yes. The tool may be used to construct a standardised BIOM file of data, that can be downloaded for any other purpose.

*Q. Should/can data from several primers/markers be combined in one table?*

*A.* Preferably, you shouldn't, but you can. DNA from the same set of samples may have amplified and sequenced with several different primer sets (e.g. COI, ITS, 16S). These should be treated as different datasets (one dataset per marker / primer-set), and each dataset should be published separately, as this makes the data maximally interoperable and reusable, and allows for calculation of relative read abundance per sample. The same sample data file may of course be (re-)used together with the different OTU tables. NB: If you have to use the tool to convert a table where data from different markers have been merged/mixed, you will need to supply the corresponding primer information etc for every single entry (OTU/ASV) in the taxon table. But the calculations of relative read abundances will be erroneous and misleading. We may look into developing a solution for this depending on wishes from the