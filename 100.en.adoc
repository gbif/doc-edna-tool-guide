= THE_TOOL – A GBIF DNA metabarcoding data conversion tool
:toc: left
:toclevels: 4

== Introduction

=== Overview

GBIF.org offers access to extensive biodiversity data, mainly focusing on where and when species have been observed. These records, sourced from collections, surveys, and citizen science, include species names, dates, locations, and  additional details. GBIF also accomodates DNA-associated biodiversity data, including  DNA metabarcoding data, and the first version of a practical dna-publishing guide was published in 2021. During consultations with the GBIF network and the research community in 2022 and 2023, it became clear that tools for easier data formatting and publishing were needed for DNA metabarcoding data.

THE_TOOL allows users with basic knowledge of data standards, GBIF and DNA data to reshape traditional dataset formats of the DNA metabarcoding community – OTU table type datasets – into standardised datasets possible to publish to GBIF. 


=== Target audiences

This guide is designed for anyone interested in publishing DNA metabarcoding data on GBIF or similar biodiversity platforms, catering to both seasoned GBIF network members and researchers new to GBIF and the data publishing process.

All new users are encouraged to review and go through both quick start guides to familiarize themselves with THE_TOOL and the processing concepts, and then consult other relevant sections of the guide and ressources as needed.

=== Scope

* The tool handles *DNA metabarcoding data* in the shape of an OTU table and associated data. The OTU table and associated data can be uploaded in a few selected templates.
* The tool does [.underline]#not# handle DNA metagenomic data (shotgun sequencing data).
* The tool does [.underline]#not# handle other types of DNA biodiversity data (specimen barcodes, qPCR)


=== Essential terms

There is a full glossary in the end of the document. The list here contains some essential terms for quick reference to new users. 

* https://dwc.tdwg.org/[Darwin Core^] (Abbr: DwC): The name of the data standard used by GBIF.
* Darwin Core [.underline]#term#: a standardised field name (e.g. term:dwc[decimalLatitude] is the official DwC term for geographical latitude).
* Occurrence: the GBIF and DwC word for a registration/detection of a species/taxon in space and time.
* Occurrence core: This is the part of DwC that includes all the central information (fields) on biological occurrences in GBIF (e.g. spatiotemporal data, taxonomy, etc), also for eDNA data.
* dna-derived data: An extension to Occurrence core to capture information relating to DNA (e.g. primers, the sequence, sequencing platform, etc.). This extension is based on the MIxS standard used by the "GenBanks". 
* OTU (Operational Taxonomic Unit): In this context it refers to the molecular units / molecular species detected in an eDNA metabarcoding study. OTUs are often a cluster of higly similar species represented by one selected sequence type.
* ASV: Amplicon sequence variant. A variant of OTUs where where only denoising of the sequence data, but no clustering, has been performed. For simplicity we refer to OTUs in general to cover also ASVs.
* OTU table: spreadsheet that holds the number of sequencing reads detected of each OTU/sequence in each sample. Here used broadly to also cover ASVs.
* Darwin Core archive (DwC-A): A standardized format for sharing biodiversity data – a ZIP file that contains one or more data files along with a descriptor file that explains how the data files are organized.
* Endpoint: In the context of GBIF, an "endpoint" refers to a URL or web address where a DwC-A can be accessed through the internet, and indexed by GBIF.

== Simple Quick Start Guide

The two quick start guides should ideally introduce most users to all features necessary to know for being able to adapt own or existing datasets to process in the tool. Start with the simple and then move to the advanced. This [.underline]#simple# quick start guide handles the processing of a minimal example dataset  with only essential explanation. It requires little or no prior knowledge about eDNA data, GBIF and DarwinCore. The intention is to familiarize the user with THE_TOOL to get basic ideas of how to adapt real datasets for processing in the tool. We recommend to follow the advanced quick start guide as the next step.

==== Simple example dataset

This quick start uses a very simple and minimalistic example datset with only 5 samples and 4 OTUs.

. Download  link:example_data/example_data1.en.xlsx[example_data_1].
. Save the file to your computer.
. (Optional) Briefly explore the structure of the example data in Excel or another app (or see detailed explanations in the section XXX ).
** The *OTU_table* sheet in the template is the OTU table. Column headers are the IDs of the 5 samples in the dataset. Row names are IDs of the 4 OTUs. Cells contain sequence read counts.
** The *Taxonomy* sheet contains the IDs of the OTUs referring to the rownames in the *OTU_table* sheet, and OTU data: The sequence, and taxonomic information derived from some reference database.
** The *Samples* sheet contains tha ID of the samples referring to the column names in the *OTU_table* sheet, and sample metadata: sample ID, latitude, longitude and date.
** The *Study* sheet contains values that are the same for the whole dataset ("global values"), in this case: the barcoding regions used, primer sequences, and primer names.
+
NOTE: all the fields (columns) of this example data are already using the 

==== Log into THE_TOOL

. Go to the website of https://edna-tool.gbif-uat.org/[THE_TOOL^].
. Log in
+
NOTE: If you do not have an account, the login prompt will link you to the sign-up form.

. Press *New Dataset* in the upper part of the page.
+
This opens the first step of the data processing.


==== Upload data (step 1)

image::process_step_1.png[]

. Drag the example dataset to the upload area, or click and select the file.
. Give the dataset a nickname (e.g. "my_first_test")
. Press *Start Upload*.
+
A green icon will indicate that the data looks OK according to some very basic data checks.
. Press *Proceed*

==== Map terms (step 2)

On this page you tell the tool what the fields in the uploaded data mean. As this test dataset already uses Darwin Core terms for the fields, no manual mapping is needed.


TIP: Press *how to use this form* to get a guided tour of this page.

* The upper section maps our sample data fields to Darwin Core terms (first column), automatically identifying and mapping four fields from the *Samples* sheet (second column) and five fields from the *Study* sheet with global values (third column) to their identically named Darwin Core counterparts. (e.g. the field containing sampling dates was called _eventDate_ in the uploaded data corresponding exactly to the Darwin Core term term:dwc[eventDate], and the field _pcr_primer_forward_ corresponding to the term term:dwc[pcr_primer_forward]).

* The second section does the same for taxonomy and sequence related information, auto-mapping four fields from the *Taxonomy* sheet to the identically named Darwin Core terms.

. Press *Proceed* to save the mapping and proceed.

==== Process data (step 3)

. Press *Process data*.
+
This produces standardized intermediate files in the BIOM format
+
NOTE: The option *assign taxonomy* uses the https://www.gbif.org/tools/sequence-id[GBIF Sequence ID tool^] to assign taxonomy to the sequences. This overwrites any taxonomy provided in the data.

. (Optional) Briefly check that number of samples and taxa are as expected (here: 5 samples and 4 taxa).
. Press *Proceed*

==== Review (step 4)

Here the data can be explored to check that everything is OK. This step is mainly intended as a sanity check of the data to ensure that control samples have been removed, and that the mapping is as expected.


. (*Optional*) Check the data.
.. Check the map and verify that the samples are placed geographically where expected (Northern part on Denmark). 
.. Check the taxonomic barchart to ensure that taxonomic composition is as expected.
.. Check ordination plots (PCoA/MDS) for outliers (any control samples that should have been excluded?).
.. Select single samples from the map or chart and explore their metadata and taxonomy in the panel to the right.
. Press *Proceed*

==== Add metadata (step 5)

On this page dataset metadata is added in a minimalistic form.

. (Mandatory) Add a meaningful title (e.g. “my first test dataset”).
. (Mandatory) Select a licence.
. (Mandatory) Add contact information - minimum: email and ORCID
+
NOTE: use e.g. 1111-2222-3333-123X as dummy ORCID if you wish.
. Leave the other fields empty.
. Press *Proceed*


==== Export (step 6)

This last page of the process produces a so-called Darwin Core archive (a zip file) that can be published directly to the GBIF test environment (UAT) from THE_TOOL. This archive can also be published properly to GBIF.org.


. Press *Create DWC archive*.
+
This creates the Darwin Core Archive from the data, going through a series of steps, that will be indicated as succesful with a green tick-mark.
. Press *Publish to GBIF test environment (UAT)*.

A prompt will inform that it takes some minutes before the data is fully ingested and will show up with all samples in the GBIF test environment. A link to the dataset in the test environment will appear next to the *Publish* button.

[start=3]
. Click on your username in the top right. Here you can:
** see your datasets,
** access them on the test environment (UAT), and
** modify and export/publish updated/new versions.

You should now have a first basic ideas of how the tool works and how you may adapt your own datasets to the template and use THE_TOOL. It is highly recommended to now go through the advanced quick start.


If you end up with a dataset suitable for publication to GBIF.org, go to <<publishing_to_gbif>>.

=== Advanced Quick Start Guide

This [.underline]#advanced# quick start guide handles the processing of a realistic example dataset with more detailed explanations of the steps and options. You should be able to follow this guide if you already used the simple quick start, but a deeper understanding of all aspects may require further reading about eDNA metabarcoding, dna-derived data, GBIF and Darwin Core data standards. The intention is to familiarize the user with the wider possibilities of THE_TOOL to be able to adapt real datasets for processing in the tool.

NOTE: Some comments and notes are not repeated from the simple quick start.

==== Advanced example dataset

This quick start quide uses a slightly modified version of a real dataset. It is from a eDNA metabarcoding study where DNA was extracted from sea water samples and amplified and sequenced for the 12S gene region. This version has been modified slightly from the original [xxx] to be able to illustrate some features in the tool and workflow, by adding fictional data and some errors.


. Download link:example_data/example_data2.en.xlsx[example_data_2].
. Save the file to your computer.
. Explore the structure of the template and example data in Excel or another app.
+
NOTE: there is a more detailed description of this example dataset here [XXX].

** The *OTU_table* sheet in the template is the OTU table. Column headers _(BAR.1, BAR.2, BAR.3, ..._) are the IDs of the 69(70) samples in the dataset. Row names (_ASV_1, ASV_2, ASV_3, ..._) are IDs of the 563 OTUs (here ASVs). Cells contain sequence read counts.
** The *Taxonomy* sheet contains the IDs of the 563 OTUs referring to the rownames in the *OTU_table* along with the sequence, and taxonomic information derived from comparing the sequences against NCBI GenBank. NB: Some fields are using DwC terms others are not yet standardised.
** The *Samples* sheet contains the IDs of the 69 samples referring to the column names in the *OTU_table* sheet, and some sample metadata: spatiotemporal data, date, etc. Some fields are using DwC terms others are not yet standardised.
** The *Study* sheet contain "sample/OTU information" that is the same for the whole dataset - e.g.: primer information, sequencing platform.

==== Log into THE_TOOL

. Go to the website of https://edna-tool.gbif-uat.org/[THE_TOOL^].
. Log in
. Press *New Dataset* in the upper part of the page.

==== Upload data (step 1)

image::img/process_step_1.png[]

. Drag the example dataset to the upload area, or click and select the file.
. Give the dataset a nickname (e.g. "my_advanced_test")
. Press *Start Upload*.
+
A green icon will indicate that the XLSX format is detected and OK according to some very basic data checks.

You will get a warning that one of the columns (samples) in the OTU table does not have a corresponding row in the sample sheet.

image::img/advanced_example_upload_warning.png[]

. Open the data viewer by clicking on the eye icon next to the uploaded dataset
+ 
Here you can see and verify the structure and content of the four sheets from the uploaded excel file.
. Scroll all the way to the right in the OTU table, and notice that there the last sample called _NEG_, wich is a negative control sample we do not want in the final dataset on GBIF.
. Click on the "Samples" tab, and scroll down and notice that this _NEG_ sample is missing (has been removed on purpose) from the sample sheet. We will leave is like that.
+
NOTE: The tool will only include samples that are present in both *Samples* and *OTU_table*, so a trick to ignore control samples is to remove them just in one place. 
. Close the viewer by pressing *Back*.
. Press *Proceed*

==== Map terms (step 2)

On this page you tell the tool which DwC terms correspond to which fields in the uploaded data.

TIP: Press *how to use this form* to get a guided tour of this page.

TIP: Press *Save mapping* once in a while to make sure that you do not get logged out and lose your work.

*First inspection*

. Inspect the overall structure and information on the page.
.. The upper section named *Sample* maps our sample data fields to Darwin Core terms (first column), automatically identifying and mapping four fields from the *Samples* sheet (second column) and five global fields from the *Study* sheet (third column) with their identically named Darwin Core counterparts.
.. The second section named *Taxon* does the same for taxonomic and sequence related information, auto-mapping four fields from the Taxon sheet to identically named Darwin Core fields.
.. The last section *Unmapped fields* lists all the fields in the uploaded data, that has names the tool do not easily recognize. Below there is an option to put unmapped fields into so-called *Extended Measurement Or Facts*.
. Press "Save Mapping" and see how you get a warning about how some essential fields have not ben mapped.

*Completing the mapping*

Starting from the top with *Sample* information, we see:

* term:dwc[id] was correctly identified and mapped.
* term:dwc[eventDate] was not found in the uploaded data, but the tool suggest to use _date_.
** click on _date_ to make this mapping.
* term:dwc[decimalLatitude] was correctly mapped.
* term:dwc[decimalLatitude] was not found in the uploaded data, and there is no suggestion.
** click on the empty field next to term:dwc[decimalLatitude], and inspect the field names from the uploaded data and notice how the latitude field was simply misspelled _ecimalLatitude_. Select it to make the mapping.
* term:mixs[target_gene] was correctly set to "12S" which was picked from the *Study* sheet containing terms with global values.
* term:mixs[otu_db] also got a value "NCBI nt" from the *Study* sheet.
* term:dwc[island] was mapped to _island_.
* term:mixs[env_medium] and the last terms of the sample section were also automatically were mapped to fields in the *study* sheet.

Now, going down to the *Taxon* section, we see:

* `id` and all the taxonomic levels were mapped automatically.
* term:mixs[DNA_sequence] was not mapped automatically, but it is suggested to use _sequence_.
** click on "sequence" to map that.

Now, going down to *Unmapped fields*, we see a series of fields (_SiteType, Depth, Location, run_accession, sample_accession, salinity_) in the uploaded data, that were not automatically identified and mapped to any Darwin Core terms.

NOTE: A detailed description the fields of the example dataset is in the section XXX.

We expect (or know) that Darwin Core can accomodate several of these un-mapped fields, and we also want to supply some global information (e.g. country), which was not included in the uploaded data.

. Go to the last part of the *Sample* section.
. Click on *Add mapping for another sample field* and look at the list of available terms.
. We wish to find some standard field to map to our field _Location_.
.. start typing "Loca" and select term:dwc[verbatimLocality].
.. click *Add field*, and see how the field is added to the list of terms.
.. Now, select our field _Location_ to map it.
. Now, we wish to map the fields with information on the corresponding sequencing files in INSDC (ENA/SRA), and follow the GBIF recommendations for which fields to use for this purpose:
.. Find and add the term term:dwc[associatedSequences] and map it to our field _run_accession_.
.. Find and add the term term:dwc[materialSampleID] and map it to our field _sample_accession_.
. To make the dataset more well documented, we will include some information that we have, but was not included in the upload.
.. All sample were from Ecuador. Add the term term:dwc[country] and type "Ecuador" in the *Add default value*.
.. We also know that all samples were from the upper layers of oceanic water.
... Add the term term:mixs[env_broad_scale].
... see how it is possible to browse the ENVO Ontology. Click and search for "epipelagic" and select the "oceanic epipelagic zone biome" with OBO ID "ENVO:01000035".
+
NOTE: this is also how the term term:mixs[env_medium] (above) is filled out from the same ontology, but this was supplied in the uploaded data.

Now, going down to *Unmapped fields*, we see that only a few fields remain unmapped. We really want to map _salinity_, but there is no standard field for that. So we will put that into *Extended Measurement Or Facts*.

. click on *salinity* from the row of unmapped fields and see how it is transferred to the the section below as a new entry.
. We know that the measurement unit is "PSU", so we add that manually.

Now, the mapping is complete.

NOTE: All available standard fields (from Occurrence Core, and the dna-derived extension) can be included in the upload files, and if spelled correctly no manual mapping is needed.

. Press *Proceed*.

==== Process data (step 3)

. Press *Process data*.
+
The tool goes through a series of steps which will be indicated as succesful with a green tick-mark, and finally produces standardized BIOM files, which the tool uses as an intermediate file format.
+
NOTE: You will get a warning that "NEG in the OTU table are not present in the SAMPLE table". We already knew that and kept it like that to exclude this negative control from the final data.
+
NOTE: The option *assign taxonomy* uses the https://www.gbif.org/tools/sequence-id[GBIF Sequence ID tool^] to assign taxonomy to the OTUs by comparing the sequences with a reference database. This overwrites any taxonomy provided in the data. If you wish to try it here, you will see that the current 12S reference database cannot assign taxonomy to a number of the sequences in this dataset project (all the non-fish). This guide assumes that you used the taxonomy in the uploaded data.

. Check that number of samples and taxa are as expected (here: 69 samples and 563 taxa).
. Press *Proceed*

==== Review (step 4)

Here the data can be explored to check that everything is OK. The options in this step are intended as sanity checks of the data to ensure that e.g. negative control samples have been removed, and that the mapping is as expected.

. Check the data.
** Check the map and verify that the samples are placed geographically where expected (Around Galapagos Islands).
** Check the taxonomic barchart to ensure that taxonomic composition is as expected.
*** try some of the other options (e.g. Absolute read abundance).
** Check ordination plots (PCoA/MDS) – that visualise compositional differnence of the samples – for outliers (any control samples that should have been excluded?).
** Select single samples from the map or from charts and explore their metadata and taxonomy in the panel to the right.
. Press *Proceed*

==== Add metadata (step 5)

On this page, dataset metadata (dataset description, persons and affiliations, etc.) is added in a minimalistic form.

TIP: toggle "Show help" to get guidance text for the fields.

. Add a meaningful title (e.g. “Fish and other vertebrates detected in sea water from the sea around Galapagos Islands, Ecuador; inferred from 12S DNA metabarcoding data with primers targeting elasmobranch.”).
. Select a licence (e.g. CC0).
. Give as rich a dataset description as you can (here just add some random text as you please).
. Add contact information - minimum is email and orcid.
+
NOTE: use e.g. 1111-2222-3333-123X as dummy orcid if you wish.
. Fill out the other fields as good as possible (or leave them empty for now).
. Press *Proceed*.

==== Export (step 6)

This last page of the process produces a Darwin Core Archive that can be published directly to the https://www.gbif-uat.org/[GBIF test environment (UAT)^] from THE_TOOL. This archive can also be published properly to GBIF.org eventually.

. Press *Create DWC archive*.
+
This creates the Darwin Core Archive from the data, going through a series of steps, that will be indicated as succesful with a green tick-mark.
. Press *Publish to GBIF test environment (UAT)*.

A prompt will inform that it takes some minutes before the data is fully ingested and will show up with all samples in the GBIF test environment (and the map will only appear the next day). A link to the dataset in the test environment will appear next to the *Publish* button.

. Explore the dataset in the test environment
. Ensure that all information and data is processed and displayed appropriately.

You should now have an good idea of how the tool works and how you may adapt your own datasets to the template and use THE_TOOL.

Be sure to check the best practices from the Detailed User Guide below.

NOTE: If you end up with a dataset suitable for publication to GBIF.org, go to <<publishing_to_gbif>>.


=== Data structure and preparation

In this section you will find detailed information and best practices on how to prepare and structure datasets for uploading and processing.

NOTE: illustrations (screenshots) may be slighlty different from what you see in the online tool, as THE_TOOL is still being developed and improved.

==== The OTU table

The tool accepts data in a format familiar to the eDNA metabarcoding community - the OTU table.

The OTU table is a specialized version of a species/site matrix, which is a standard way to organize data in biodiveristy and ecology studies, summarizing the presence and abundance of different organisms across various samples. In an OTU table each row represents a unique OTU, and each column represents a different sample or site, with the cell values indicating the count of sequencing reads of the OTUs in each sample. An OTU table may also have OTUs as columns and samples as rows. Here, we are using the term OTU broadly to cover also ASVs and other types of representative sequences resulting from bioinformatic processing of raw sequencing data from DNA metabarcoding.

The two dimensions (columns and rows) of the central OTU table is most often associated with two other tables:

* One relating to the samples, e.g. geographical position and sampling date.
* One relating to the OTUs: at least the sequence, but likely also taxonomy.

image::img/OTU_table_all_filed.png[]
.The OTU table (example dataset 1). A minimalistic example of an *OTU_table* with 5 samples (Sample_A, Sample_B,..) as columns and 4 OTUs (OTU_001, OTU_002,..) as rows. Green boxes and arrow indicate the linking of the OTUs by their IDs to second table *Taxonomy* with taxonomic/sequence data data. Blue boxes and arrow indicate the the linling of samples by their IDs to the third table *Samples* with and sample metadata. The optional *Study* and *Seqs.fasta* are explained below. 

NOTE: 
It is commonly seen that an OTU_table with OTUs as rows has taxonomic information of the OTUs (sequence, kingdom, phylum, etc) added as extra columns. More rarely we see that sample metadata (position, sampling date, soil ph, etc) is added as extra columns to an OTU_table having samples as rows.

Most analysed datasets from DNA metabarcoding will be possible to fit into this three-table format.

To be able to publish biodiversity data in GBIF, the data needs to be formatted as a list of occurrences. As this reshaping can be error prone and complicated, THE_TOOL has been build to automate this process. In other terms we use the OTU table as a publishing model, targeting a familiar data format of this research community. 

With THE_TOOL there is provided number of dataset templates. They all contain 3-4 tables (as sheets in an Excel workbook or as separate tsv/csv files) and potentially a separate file with the sequences:

* *OTU_table*: The table (matrix) of sequence read counts of each OTU in each sample
* *Taxonomy*: a table of the sequences and potentially their taxonomy, and rarely much more, as relevant metadata (sequencing platform, primers) can be put into the *Study* table.
* *Samples*: a table of the metadata associated with the samples. This table is where all the metadata with different values for each sample is placed (geography, sampling date, etc.).
* *Study*: this (optional) fourth table contain only two columns (term, value). This table can be used for metadata (terms) with values that are the same for the whole dataset ("global values") – often that would be things like: target gene, primer information, sequencing platform, pipeline, extraction procedure, etc.
* *Seqs.fasta*: this (optional) file may contain the sequences of the OTUs instead of having them as a field in the *Taxonomy* table. 

==== Preparing a dataset

===== Basic checks

* The data is DNA metabarcoding data (amplified and sequenced marker genes) and not something else (metagenomics, specimen barcoding, qPCR)
* You have access to the processed data as OTU table and associated table (and not just access to the raw un-amalysed sequence data - e.g. fastq files)
* You are allowed to share the data in GBIF.
* You have the most essential data available
** location, date for the samples
** sequences of the OTUs/ASVs
** an OTU table with number of sequence reads of each OTU in each sample
** some information that allows you to contruct a dataset description
** information on the persons that should be associated as creators and contacts for the dataset. 

===== Select a template

Select one of the available data structures (templates) that is suitable for your data.

* If you prefer Excel workbooks, then use template 1 og 2.
* If you prefer tab (or comma) separated text files, go for template 3 or 4. 
* If you have the sequence reads as a separate fasta file, then use template 2 or 4.

===== Fit the data into the template

Now try to fit the data into the selected template and provide as many as the  <<recommended, required and recommended fields>>.

TIP: The order (sorting) of rows and columns in tables is not important. 

*OTU_table*

* Fit your data into an *OTU_table* with samples as columns and OTUs as row.
* Remove rows and columns that are not samples and OTUs.
** No rows or columns with sums/totals.
** No columns with sample or taxomomy data, move that to a separate table/sheet.
* Use 0 (zero) in cells without a read count (not "NA", "-" or similar).
* Make sure that samples IDs (column headers) are unique, and prefarably do not contain a lot of unstandardized [XXX] characters.
* Make sure that OTU IDs (row names) are unique, and prefarably do not contain a lot of unstandardized [XXX] characters.
* Leave the upper left cell empty.
+
NOTE: This table does not hold any DwC fields/terms

*Taxonomy*

* Fit your OTU data into a *Taxonomy* table with OTUs as rows and associated data as columns.
* First colums should be `id` and contain OTU IDs referring to (identical to) the row names in the *OTU_table*.
* Normally this table would not contain a lot of fields, only those that relate to the single OTUs and carry different values per OTU:
** The sequence (as term:dwc[DNA_sequence]) unless these are provided in a separate fasta file.
** Taxonomy inferred from comparing the sequences againat a reference database
*** This can be given as a XXXX
+
NOTE: most metadata related to sequences (sequencing platform, primers, etc) is identical for all samples and OTUs, and these can be uploaded in the fouth table *Study* with "global" values. 
* Use Darwin Core terms for your fields to minimize manual mapping in later steps.

*Samples*

* Fit your Sample metadata into a *Sample* table with Sample IDs as rows and associated data as columns.
* First colums should be `id` and contain Sample IDs referring to (identical to) the column names in the *OTU_table*.
* This table is where you fit all the metadata related to the single samples if the values are different between samples
** Sampling locations
** Sampling dates
** Physical properties related to the sample (e.g. pH)
** Links to sample-associated data elsewhere – e.g. raw sequence data and biosample record in INSDC.
* Be sure to include as many of the required and recommended fields as possible (see below).
* There are many more fields to select from in the Occurrence Core (LINK) and dna-derived extension (link).
+
TIP: Use *Study* table for fields/terms that does not differ between samples.
* Use Darwin Core terms for your fields to minimize manual mapping in later steps. 

*Study* (optional)

The use of this table is optional. But as many data fileds are often applicable to the whole study, we recommend to use this table (as opposed to have the same fields in the *Samples* table with identical values for all entries.

* Fit all metadata fields with global values (same value for all samples and/or OTUs) into this table.
* the table has two columns (_term_, _value_). Each row holds any term from Darwin Core (incl the dna-derived extension) in the _term_ field, and the corresponding _value_ contains the value relevant for this study (see minimal example below).
* Be sure to include as many of the required and recommended fields as possible (see below).
* Use Darwin Core terms for your fields to minimize manual mapping in later steps.

*Seqs.fasta* (optional)

In some metabarcoding datasets, the sequences are placed in a separate https://en.wikipedia.org/wiki/FASTA_format[fasta file^]. This is also possible here. If this option is chosen, then a few checks are good.

* Make sure you follow the classic fasta formatting. A sequence begins with a greater-than character (">") followed by the OTU ID. The lines immediately following this header line are the sequence representation (ACTG...). The next ">" marks the beginning of the next sequence.
* OTU IDs are used as headers in the fasta file.
* OTU IDs should be the same as those in the OTU table.

image:img/Fasta_file_example.png[]

*Dataset descriptions, people and other metadata*

Before starting the data upload and processing in THE_TOOL it is a good idea to prepare a dataset description, collect information on the people you need to associate with the data, etc.

You need to prepare:
* A dataset title. Choose something descriptive. Examples:
** XXX
** XXX
** XXX
* A dataset description. XXXX
* Persons that should be associated with the dataset. This could be the authors of an associated research paper, the laboratory personnel, the person preparing the dataset for GBIF publication, etc.
** Name
** Affiliation
** Address
** email
** orcid
* All persons added will be listed as authors in the suggested dataset citation.
* Be sure to designate one of the persons (you?) as the contact person. This is the person that will be contacted if e.g. users find issues in the data.
* Associated ressources ???

NOTE: THE_TOOL uses a simple form for providing this dataset metadata. This is intentional as we do not wish the demotivate users by confronting them with a form with many options only marginally relevant for DNA metabarcoding data. If you chose to publish the processed data through an IPT, you will have the possibility of adding metadata there.

==== Required and recommended fields [[recommended]]

This section contains a list of required and recommended fields. These tables are modified/specialized version of tables xxx in the dna-publishing guide xxx specifically for when using THE_TOOL. 

When using THE_TOOL it is not important to know whether the fields you are using are from Occurrence Core or the dna-derived extension. But it is important to know in which table to put the fields and associated values. The *Placement* column explains where to provide each field. A lot of the required and recommended fields are automatically handled/calculated, and do should not be provided by the user – *Placement* informs about this also.

NOTE: many of the fields relating to taxonomy/sequences (e.g. primers, reference database) should be possible to give as global values in the *Study* table. If you find that this is not the case, you may be dealing with a mixed dataset, with sequences from more than one primer set (e.g. COI and 16S sequences from the same set of samples). We recommend to publish such datasets separately.

[[table-01]]
.Recommended fields for http://rs.gbif.org/core/dwc_occurrence_2020-04-15.xml[Occurrence core] for Metabarcoding data. This table is a modified version of table xxx in the dna-publishing guide xxx specifically for using THE_TOOL.
[cols="1,1,4,1,1",options="header"]
|===
| Field name
| Examples / explanation
| Description
| Required
| Placement

| term:dwc[basisOfRecord]
| _This field is is automatically set as "Material Sample" by the tool_
| The specific nature of the data record - a subtype of the http://rs.gbif.org/vocabulary/dwc/basis_of_record.xml[dcterms:type].
| Required
| _Automatic by tool_

| term:dwc[occurrenceID]
| _This field is automatilly contructed by the tool as "eventID:OTU_id"_
| A unique identifier for the occurrence, allowing the same occurrence to be recognized across dataset versions as well as through data downloads and use.
| Required
| _Automatic by tool_

| term:dwc[eventID]
| _This field is automatically set to the IDs of the Sample_
| An identifier for the set of information associated with an Event (something that occurs at a place and time).
| Highly recommended
| _Automatic by tool_

| term:dwc[eventDate]
| 2020-01-05
| Date when the event was recorded. Recommended best practice is to use a date that conforms to ISO 8601-1:2019. For more information, check https://dwc.tdwg.org/terms/#dwc:eventDate
| Required
| *Samples* (or *Study*)

| term:dwc[recordedBy]
| "Oliver P. Pearson \| Anita K. Pearson"
| A list (concatenated and separated) of names of people, groups, or organizations responsible for recording the original Occurrence. The recommended best practice is to separate the values with a vertical bar (' \| '). Including information about the observer improves the scientific reproducibility (https://doi.org/10.1093/database/baaa072[Groom et al. 2020^]).
| Highly recommended
| *Samples* or *Study*

| term:dwc[organismQuantity]
| _This field is automatically filled with the value from the corresponding cell in the uploaded OTU table_
| Number of reads of this OTU or ASV in the sample.
| Highly recommended
| _Automatic by tool_

| term:dwc[organismQuantityType]
| _This field is automatically filled with the value "DNA sequence reads"_
| Should always be “DNA sequence reads”
| Highly recommended
| _Automatic by tool_

| term:dwc[sampleSizeValue]
| _This field is automatically filled total number of reads in the sample as calculated by the tool automatically_
| Total number of reads in the sample. This is important since it allows calculating the relative abundance of each OTU or ASV within the sample.
| Highly recommended
| _Automatic by tool_

| term:dwc[sampleSizeUnit]
| DNA sequence reads
| _This field is automatically filled with the value “DNA sequence reads”_
| Highly recommended
| _Automatic by tool_

| term:dwc[materialSampleID]
| https://www.ncbi.nlm.nih.gov/biosample/15224856 +
 +
https://www.ebi.ac.uk/ena/browser/view/SAMEA3724543 +
 +
urn:uuid:a964805b-33c2-439a-beaa-6379ebbfcd03
| An identifier for the MaterialSample (as opposed to a particular digital record of the material sample). Use the biosample ID if one was obtained from a nucleotide archive. In the absence of a persistent global unique identifier, construct one from a combination of identifiers in the record that will most closely make the materialSampleID globally unique.
| Highly recommended
| *Samples*

| term:dwc[samplingProtocol]
| UV light trap
| The name of, reference to, or description of the method or protocol used during a sampling Event. https://dwc.tdwg.org/terms/#dwc:samplingProtocol
| Recommended
| *Study* (or *Samples*)

| term:dwc[associatedSequences]
| https://www.ebi.ac.uk/ena/browser/view/ERR1202046
| A list (concatenated and separated) of identifiers (publication, global unique identifier, URI). For most cases it woule be linking to archived raw metabarcoding read files in a public repository.
| Recommended
| *Samples*

| term:dwc[identificationRemarks]
| RDP annotation confidence (at lowest specified taxon): 0.96, against reference database: GTDB
| Specification of taxonomic identification process, ideally including data on applied algorithm and reference database, as well as on level of confidence in the resulting identification.
| Recommended
| *Study* (or *Taxonomy*)

| term:dwc[identificationReferences]
| https://www.ebi.ac.uk/metagenomics/pipelines/4.1 + 
 +
https://github.com/terrimporter/CO1Classifier
| A list (concatenated and separated) of references (publication, global unique identifier, URI) used in the Identification. Recommended best practice is to separate the values in a list with space vertical bar space ( \| ).
| Recommended
| *Study* (or *Taxonomy*)

| term:dwc[decimalLatitude]
| 60.545207
| The geographic latitude (in decimal degrees, using the spatial reference system given in geodeticDatum) of the geographic centre of a Location. Positive values are north of the Equator, negative values are south of it. Legal values lie between -90 and 90, inclusive.
| Highly recommended
| *Samples* (or *Study*)

| term:dwc[decimalLongitude]
| 24.174556
| The geographic longitude (in decimal degrees, using the spatial reference system given in geodeticDatum) of the geographic centre of a Location. Positive values are east of the Greenwich Meridian, negative values are west of it. Legal values lie between -180 and 180, inclusive.
| Highly recommended
| *Samples* (or *Study*)

// The [.break-all]#ASV:…# is to allow the identifier to be broken at any character, rather than stretching the text cell.
| term:dwc[taxonID]
| _This field is automatically filled with an MD5 hash of the sequence – e.g. [.break-all]#ASV:7bdb57487bee022ba30c03c3e7ca50e1#_
| For eDNA data, it is recommended to use an MD5 hash of the sequence and prepend it with “ASV:”. See also <<taxonomy-of-sequences>>.
| Highly recommended
| _Automatic by tool_

| term:dwc[scientificName]
| _Gadus morhua_ L. 1758, BOLD:ACF1143
| Scientific name of the closest known taxon (species or higher) or an OTU identifier from BOLD (BIN) or UNITE (SH)
| Required (filled with "Incertae sedis" if left blank)
| *Taxonomy*

| term:dwc[kingdom]
| Animalia
| Higher taxonomy
| Highly recommended
| *Taxonomy*

| term:dwc[phylum]
| Chordata
| Higher taxonomy
| Recommended
| *Taxonomy*

| term:dwc[class]
| Actinopterygii
| Higher taxonomy
| Recommended
| *Taxonomy*

| term:dwc[order]
| Gadiformes
| Higher taxonomy
| Recommended
| *Taxonomy*

| term:dwc[family]
| Gadidae
| Higher taxonomy
| Recommended
| *Taxonomy*

| term:dwc[genus]
| _Gadus_
| Higher taxonomy
| Recommended
| *Taxonomy*

|===

<<<

[[table-02]]
.Recommended fields from the DNA derived data extension (a selection) for metabarcoding data
[cols="1,1,4,1,1",options="header"]
|===
| Field name
| Examples
| Description
| Required
| Placement

// The [.break-all]#TCTA…# is to allow the sequence to be broken at any character, rather than stretching the text cell.
| term:mixs[DNA_sequence]
| [.break-all]#TCTATCCTCAATTATAGGTCATAATTCACCATCAGTAGATTTAGGAATTTTCTCTATTCATATTGCAGGTGTATCATCAATTATAGGATCAATTAATTTTATTGTAACAATTTTAAATATACATACAAAAACTCATTCATTAAACTTTTTACCATTATTTTCATGATCAGTTCTAGTTACAGCAATTCTCCTTTTATTATCATTA#
| The DNA sequence (ASV). Taxonomic interpretation of the sequence depends on the technology and reference library available at the time of publication. Hence, the most objective taxonomic handle is the sequence which can be reinterpreted in the future.
| Required (Highly recommended)
| *Taxonomy* or in separate fasta file (*Seqs.fasta*)

| term:mixs[sop]
| https://www.protocols.io/view/emp-its-illumina-amplicon-protocol-pa7dihn
| Standard operating procedures used in assembly and/or annotation of genomes, metagenomes or environmental sequences. +
 +
A reference to a well documented protocol, e.g. using https://protocols.io[protocols.io]
| Recommended
| *Study*

| term:mixs[target_gene]
| 16S rRNA, 18S rRNA, ITS
| Targeted gene or marker name for marker-based studies
| Highly recommended
| *Study*

| term:mixs[target_subfragment]
| V6, V9, ITS2
| Name of subfragment of a gene or markerImportant to e.g. identify special regions on marker genes like the hypervariable V6 region of the 16S rRNA gene
| Highly recommended
| *Study*

| term:mixs[pcr_primer_forward]
| GGACTACHVGGGTWTCTAAT
| Forward PCR primer that was used to amplify the sequence of the targeted gene, locus or subfragment.
| Highly recommended
| *Study*

| term:mixs[pcr_primer_reverse]
| GGACTACHVGGGTWTCTAAT
| Reverse PCR primer that was used to amplify the sequence of the targeted gene, locus or subfragment.
| Highly recommended
| *Study*

| term:mixs[pcr_primer_name_forward]
| jgLCO1490
| Name of the forward PCR primer
| Highly recommended
| *Study*

| term:mixs[pcr_primer_name_reverse]
| jgHCO2198
| Name of the reverse PCR primer
| Highly recommended
| *Study*

| term:mixs[pcr_primer_reference]
| https://doi.org/10.1186/1742-9994-10-34
| Reference for the primers
| Highly recommended
| *Study*

| term:mixs[env_broad_scale]
| forest biome [ENVO:01000174]
| *Equivalent to env_biome in MIxS v4* +
In this field, report which major environmental system your sample or specimen came from. The systems identified should have a coarse spatial grain, to provide the general environmental context of where the sampling was done (e.g. were you in the desert or a rainforest?). We recommend using subclasses of ENVO’s biome class: +
http://purl.obolibrary.org/obo/ENVO_00000428
| Recommended (ENVO can be browsed and selected interactively in tool)
| *Samples*

| term:mixs[env_local_scale]
| litter layer [ENVO:01000338]
| *Equivalent to env_feature in MIxS v4* +
In this field, report the entity or entities which are in your sample or specimen´s local vicinity and which you believe have significant causal influences on your sample or specimen. Please use terms that are present in ENVO and which are of smaller spatial grain than your entry for env_broad_scale.
| Recommended (ENVO can be browsed and selected interactively in tool)
| *Samples*

| term:mixs[env_medium]
| soil[ENVO:00001998]
| *Equivalent to env_material in MIxS v4* +
In this field, report which environmental material or materials (pipe separated) immediately surrounded your sample or specimen prior to sampling, using one or more subclasses of ENVO´s environmental material class: +
http://purl.obolibrary.org/obo/ENVO_00010483
| Recommended (ENVO can be browsed and selected interactively in tool)
| *Samples*

| term:mixs[lib_layout]
| Paired
| *Equivalent to lib_const_meth in MIxS v4* +
Specify whether to expect single, paired, or other configuration of reads
| Recommended
| *Samples*

| term:mixs[seq_meth]
| Illumina HiSeq 1500
| Sequencing method/platform used
| Highly recommended
| *Study*

| term:mixs[otu_class_appr]
| "dada2; 1.14.0; ASV"
| Approach/algorithm and clustering level (if relevant) when defining OTUs or ASVs
| Highly recommended
| *Study*

| term:mixs[otu_seq_comp_appr]
| "blastn;2.6.0+;e-value cutoff: 0.001"
| Tool and thresholds used to assign "species-level" names to OTUs or ASVs
| Highly recommended
| *Study*

| term:mixs[otu_db]
| "Genbank nr;221", "UNITE;8.2"
| Reference database (i.e. sequences not generated as part of the current study) used to assigning taxonomy to OTUs or ASVs
| Highly recommended
| *Study*
|===

<<<

=== Dataset Templates

==== Data template 1 – Excel workbook

A workbook with three/four sheets: *OTU_table*, *Taxonomy*, *Samples* and (optionally) *Study*. The latter (*Study*) contains study-wide global values. The *OTU_table* must have samples as columns and OTUs as rows.

==== Data template 2 – Excel workbook with fasta file

A workbook with three/four sheets: *OTU_table*, *Taxonomy*, *Samples* and (optionally) *Study*. The latter (*Study*) contains study-wide global values. Instead of having the sequences in the *Taxonomy* sheet, a fasta file (*seqs.fasta*) is provided. The *OTU_table* must have samples as columns and OTUs as rows. The fasta file must have fasta headers corresponding to the OTU IDs used in the *OTU_table* sheet and in the *Taxonomy* sheet.

==== Data template 3 – TSV format

Three/four tsv (or csv) files *OTU_table.tsv*, *Taxonomy.tsv*, *Samples.tsv*, and (optionally) *Study.tsv* containing study-wide global values. The OTU table can have samples as either columns or rows.

==== Data template 4 – TSV format with fasta file

Three/four tsv (or csv) files *OTU_table.tsv*, *Taxonomy.tsv*, *Samples.tsv*, and (optionally) *Study.tsv* containing study-wide global values. Instead of having the sequences in the *Taxonomy.tsv*, a fasta file (*seqs.fasta*) is provided. The *OTU_table.tsv* can have samples as either columns or rows. The fasta file must have fasta headers corresponding to the OTU IDs used in the *OTU_table.tsv* and in the *Taxonomy.tsv*.

=== 

== Publishing datasets from THE_TOOL to GBIF.org [[publishing_to_gbif]]

== FAQ

*Q. What is the state of this tool?*  

*A.* This is a prototype, and is beeing continuously developed. This means, that you may encounter bugs and problems that we have not yet addressed. You will be able to make a Darwin Core archive and download it, but you will not be able to publish it directly to GBIF.org through THE_TOOL. If you encounter bugs, inconveniences, have concrete input or want to request a feature, please make a github issue using the links on website of THE_TOOL.

*Q. What does THE_TOOL do?*

*A.* It helps format a DNA metabarcoding dataset (OTU table style) to be published on GBIF.org without the user having to learn Darwin Core terms and know a lot about data standardisation and reformating. It performs a transformation of the familar OTU table (with associated sample info & taxonomic/sequence informantion) into a tall table, where each row reflects one occurrence – a taxon (sequence/OTU/ASV) in time and space – and facilitates the mapping/renaming of user-named field names to the biodiversity standard, DarwinCore. These are all steps that can be done manually following the DNA publishing guide [LINK], but THE_TOOL makes it easier.

*Q. Who can use the tool?*

*A.* Anybody. 

*Q. Are there templates?*

*A.* Yes, there are a few templates and also example datasets. See above (XXX).

*Q. What kind of data can be published/submitted using this tool?*

*A.* This tool processes an OTU table so the data can be published to GBIF.org. With an OTU table we think of a table containing some amplified marker gene sequences (ASVs/OTUs) and their sequence abundance in a set of samples. Each sample corresponds to an environmental sample or bulk sample (air, soil, water, faeces, insect trap homogenate, gut contents, ...), from which DNA has been extracted. A selected genetic region (barcode region) has been amplified with selected primers and sequenced on a high throughput seqeuncing platform like Illumina MiSeq.

*Q. Can the tool be used metagenomic datasets?*

*A.* No. However note, that there is a confusion about the terms "metagenomic" and "metabarcoding". Metagenomic data sequences and captures all genetic material from an environmental sample, often with so-called shotgun sequencing. Metabarcoding data sequences specific selected DNA regions often called barcoding regions (e.g. CO1, ITS, 18S, 16S) to identify species in a sample, focusing on community composition. So, although the microbial research community often labels 16S amplicon sequencing (16S metabarcoding) as "metagenomic", that type of data would be suitable for processing in this tool, as it is associated with 16S sequences only. 

*Q. What kind of DNA metabarcoding samples are acceptable to publish on GBIF.org?*

*A.* eDNA metabarcoding based data from all environmental samples (soil, air, water, dust, etc) as well as bulk samples of small organisms (e.g. from malaise trap) are acceptable. Heavily manipulated/treated environmental samples may not reflect real biodiversity and deemed as irrelevant from a biodiversity perspective. Use your judgement.

*Q. Which markers/barcodes (COI, ITS, 16S,..) does GBIF and the tool support?*

*A.* It is possible to publish data based on amplification and sequencing of any amplified barcoding region. 

*Q. Should sequences be trimmed?*

*A.* Primers, adapters and tags, etc should always be removed from sequences. If you have trimmed your sequences further (e.g. trimming away the end of 5.8S and start of 28S from ITS2 data), then that is also acceptable, but not a requirement.

*Q. Should sequences be clustered into OTUs?*

*A.* 100% identical sequences should always be collapsed (dereplicated), and futher clustering, denoising and compression may be relevant depending of sequencing platform and bioinformatic tools used. If using e.g. the Illumina MiSeq platform, we recommend sharing unclustered (but denoised) amplicon sequence variants (ASVs). This approach keeps the data maximally interoperable with data from other studies, compared to clusting into broader (e.g. 97% culstering) OTUs, where centroids (the variant picked to represent an OTU) of almost similar OTUs may have been picked differently between datasets and algorithms.


*Q. Should sequence read abundance be converted to relative abundance?*

*A.* No. GBIF recommends to share detected absolute sequence read abundance (detected number of reads of each ASV/OTU in each sample). The tool will automatically calculate the total number of reads per sample and relative abundance, so that future users will have the option to filter on both absolute and relative abundance.

*Q. Should samples be resampled/rarefied to even sequencing depth?*

*A.* No. When doing metabarcoding, researchers are often resampling the OTU tables to achieve even sequencing depth (same total number of reads per sample) to standardise sampling effort across samples. GBIF recommends to share detected absolute abundances (number of reads per ASV/OTU in each sample). The tool will automatically calculate total number of reads per sample and relative abundances, so that future users have the option to filter on both absolute and relative abundances. Users downloading whole datasets will be able to do this resampling themselves if they wish.

*Q. Should negative controls, positive controls, blanks and failed samples be removed from the dataset?*

*A.* Yes. Only share data from real environmental samples producing data that seems trustworthy should be shared. NB: The tool only includes samples that are present in both the sample data AND the OTU table - i.e. it automatically discards samples that are absent from either table. So, removing controls from the sample-list is an easy way to do that.


*Q. Should I remove singletons, infrequent or low abundant sequences?*

*A.* No. There may be a good reason to remove low abundant sequences, singletons, infrequent sequences in some studies. But GBIF does not recommend any default removal of singletons, infrequent og low abundant sequences.

*Q. Should data from replicates be merged?*

*A.* Maybe. Do what makes the data most suitable for reuse in biodiversity studies. If replication (multiple samples, DNA extractions, PCRs) was used to reduce stochasticity, then (bioinformatic) merging of replicates may be a good choice.

*Q. What if there are several versions of an OTU table?*

*A.* Only one verison of the OTU table should be shared. Sometimes several version of an OTU table exist - e.g. clustered at different thresholds, removed non-target species and suspected contaminants - or split it into several tables with different taxonomic scopes. GBIF recommends to share the most inclusive version, including everything detected.

*Q. Should data from suspected contaminants be removed?*

*A.* Yes. Some sequences/OTUs may be suspected contamination (e.g. DNA from human and classical food items like tomato, potato, chicken, etc.). We recommend to remove these if they can be identified. Only taxa/OTUs that are present in both the taxon table AND the OTU table will be processed. So, removing suspected contaminats from the taxon information is an easy way to do that.

*Q. Should non-target sequences be removed?*

*A.* Not necessarily. Some sequences/OTUs are perceived as non-target sequences - e.g. if mammals are detected in a study using fish-specific primers. However, most of those non-target sequences may still be biodiversity relevant data seen in a larger perspective. Also, such custom filterings of data may actually make the data less compatible with similar datasets produced with the same primers, and it makes the calculation of relative read abundances flawed. So, GBIF generally encourages not to remove non-target sequences, unless they are obviously contaminations or otherwise untrustworthy.

*Q. Should taxonomy be assigned to sequences?*

*A.* Not necessarily. Currently GBIF identifies/indexes data based on the taxonomy you provide. If only the sequence is provided, the inferred occurrences will be stored under the label "incertae sedis" for now. However, the presence of the sequence will make it possible to assign taxonomy at a later stage. GBIF aims to provide the possibility of automatic updating of sequence based identification (see above). The tool currently also allows assigning of taxonomy for a few genetic markers and organism groups.

*Q. How should taxonomy be assigned to sequences?*

*A.* There are many reference databases and tools for assigning taxonomy to sequences, and reference databases are continuously being improved and changed. GBIF does not recommend any particular tool or pipeline. Use what is appropriate for the data. GBIF provides a sequence annotation tool for some markers. You can use that if you wish. The sequence ID tool is also built into this eDNA data converter tool as an option during the processing step, but as this step takes time you may want to use the sequence ID tool alone before using using this conveter. [NB: In the long term GBIF hopes to be able to continuously reannotate sequence based data to ensure consistency across datasets and time. GBIF will however keep original taxonomic identifications provided by the user to ensure traceability.]

*Q. How should I provide the taxonomic information when I submit my OTU data to GBIF?*

*A.* Take a look at the template (link above XXXX).

*Q. Should I share sequences that cannot be taxonomically identified?*

*A.* Yes. By default all OTUs/ASVs should be shared. Sequences that cannot be reliably identified to species level (or to genus, or any taxonomic level at all) generally reflect the fact that reference databases are incomplete and/or not 100% curated. However, as reference databases are continuously improved, many sequences will be possible to receive improved taxonomic affiliation. So please provide all sequences.

*Q. Will GBIF make sure that the taxonomy is updated?*

*A.* Hopefully yes. For many barcoding regions and taxonomic groups, reference databases are incomplete and partially incorrect, but continuously improved. Thus, taxonomic identifications based on comparison with reference databases often reflect the current state of the database used. In the long term GBIF aims to continuously reannotate sequence based data to ensure consistency across datasets and time. GBIF will keep original taxonomic identifications provided by the user to ensure traceability.

*Q. How does GBIF ensure fitness for reuse and interoperability of data?*

*A.* In the long term GBIF aims to continuously re-annotate sequence based data to ensure consistency across datasets and time. GBIF will however keep original taxonomic identifications provided by the user to ensure traceability. GBIF is also working on better tools for searching for and filtering of sequence based data.

*Q. Can the tool be used to just to make a Darwin Core archive?*

*A.* Yes. The tool can be used to produce a Darwin Core archive. This darwin core archive can then be published to GBIF, OBIS or another research infrastructure through another publishing process.

*Q. Can the tool be used to just to make a BIOM file?*

*A.* Yes. The tool may be used to construct a standardised BIOM file of data, that can be downloaded for any other purpose.

*Q. Should/can data from several primers/markers be combined in one table?*

*A.* Preferably, you shouldn't, but you can. DNA from the same set of samples may have amplified and sequenced with several different primer sets (e.g. COI, ITS, 16S). These should be treated as different datasets (one dataset per marker / primer-set), and each dataset should be published separately, as this makes the data maximally interoperable and reusable, and allows for calculation of relative read abundance per sample. The same sample data file may of course be (re-)used together with the different OTU tables. NB: If you have to use the tool to convert a table where data from different markers have been merged/mixed, you will need to supply the corresponding primer information etc for every single entry (OTU/ASV) in the taxon table. But the calculations of relative read abundances will be erroneous and misleading. We may look into developing a solution for this depending on wishes from the